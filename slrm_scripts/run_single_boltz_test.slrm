#!/bin/bash
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=32
#SBATCH --gpus-per-node=4
#SBATCH --mem=128GB
#SBATCH --partition=kempner_requeue
#SBATCH --account=kempner_bsabatini_lab
#SBATCH --time=02:00:00
#SBATCH --mail-type=ALL
#SBATCH --mail-user=thomasbush52@gmail.com
# Use array-aware log names to avoid clobbering:
#SBATCH --output=/n/home06/tbush/job_logs/%x.%A_%a.out

set -euo pipefail
INPUT_FILE=$1
OUTPUT_DIR=$2

BOLTZ_CACHE="/n/holylfs06/LABS/kempner_shared/Everyone/workflow/boltz/boltz_db"
BOLTZ_OUTPUT_DIR="${OUTPUT_DIR}/boltz_output/"
export TRITON_CACHE_DIR="${SLURM_TMPDIR:-/tmp}/triton_cache_${USER}_single_job"
mkdir -p "$TRITON_CACHE_DIR"
mkdir -p "$BOLTZ_OUTPUT_DIR"
module load python/3.12.8-fasrc01 gcc/14.2.0-fasrc01 cuda/12.9.1-fasrc01 cudnn/9.10.2.21_cuda12-fasrc01
export PATH="/n/holylfs06/LABS/kempner_shared/Everyone/common_envs/miniconda3/envs/boltz/localcolabfold/colabfold-conda/bin:$PATH"
mamba activate /n/holylfs06/LABS/kempner_shared/Everyone/common_envs/miniconda3/envs/boltz
cat > boltz_spawn.py <<'PY'
import torch.multiprocessing as mp
mp.set_start_method("spawn", force=True)

import runpy
runpy.run_module("boltz", run_name="__main__")
PY

# make predictions
echo "Starting boltz predictions "
if ! boltz predict "$INPUT_FILE" --cache "$BOLTZ_CACHE" --out_dir "$BOLTZ_OUTPUT_DIR" --devices 4 --accelerator gpu --recycling_steps 10 --diffusion_samples 25 --num_workers 0 --override; then
  boltz predict "$INPUT_FILE" --cache "$BOLTZ_CACHE" --out_dir "$BOLTZ_OUTPUT_DIR" --devices 4 --accelerator gpu --recycling_steps 10 --diffusion_samples 25 --num_workers 0 --override --no_kernels
fi

echo "Boltz predictions compled."
